{"name":"Cs205-finalproject","tagline":"CS205 final project, parallel computation","body":"# Project Motivation: Problem We Solve\r\n\tIn the world today, there has been an explosion of content generation that has began to force us to think more critically about how we can best organize our data. Every single day, more than 92,000 articles are posted to the web <sup> 1 </sup>, leaving social media websites, news aggregators and the like scrambling to figure out what to best serve you and keep you engaged. In addition to helping ensure that users get content that is tailored to their individual interests, clustering such a corpus by content similarity is critical to ensure that the massive amount of redundancy in these articles aren’t repeated in a user’s feed and that they get a diverse set of topics to explore. In another sense, if we find a topic that a user would like to read more on, clustering also enables us to delve deeper into that category and discover more written on it. \r\nUnfortunately, the scale which we have described here does not lend itself well to \r\ntraditional clustering algorithms as they are extremely computationally intensive and thus cannot run in near real-time (a property we would desire if we were to deploy any of these into a real-world application today). Such algorithms typically run by:\r\nConverting each document in the corpus into a vector. We use TFIDF for this, which will be described later. \r\nCalculating either similarities (i.e. cosine similarity) or distances between vectors to have a notion of “relatedness”. \r\nUse a clustering algorithm that then uses the measure computed before to organize our corpus. We use DBSCAN for this aspect, an award winning algorithm that does density-based clustering.\r\nWith a large number of documents all of these steps, and especially calculating similarity/distance, takes a large amount of time. Thus, we decided this would be an ideal problem for us to optimize using parallelism and other techniques (such as dimensionality reduction) to try to bring to near real-time performance so that our implementation could allow a user could feed in a large set of documents into our system and see the clusters and categories that result.\r\n\r\n\r\nData Described:\r\n\tIn lieu of articles, our corpus of documents consisted of posts that we pulled from Stack Overflow using their API. We did this because we were initially just interested in doing tag recommendation for questions users had posted, but after working on our project for a few weeks realized that our approach could be used to solve an even harder problem, namely clustering large sets of documents efficiently. Also, the posts were well-formated within the API, we had access to a large number of them, and there were many logical comparisons and similarities to make across posts (i.e. posts about a bug versus posts about style questions, etc). In order to get access to Stack Overflow’s API, we registered ourselves with their system, received an API key and then proceeded to pull 540,000 posts and their related attributes from the website. This was an extremely large set to work with and place into memory, and so we often worked with a subset of the posts of between 20,000 to 40,000 questions. \r\n\r\n\r\nAlthough the posts had several attributes (such as author, score, etc), we focused on just the body of the post as we wanted our approach to be extensible for all other forms of text content. Once we loaded in the questions, we did some post-processing to \r\n//TODO: SAMI -- Discuss briefly the post-processing involved in stemming, etc. \r\nGeorge knows better here, beautiful soup and shit\r\n\r\nInsights:\r\n[This section should come after “Results” aka “Performance of your code”]\r\n\tOverall, we were extremely pleased with the outcomes of our projects and accordingly have a number of insights we are excited to share. \r\n\r\n\r\nExtent of Speed-Ups\r\nTo start with, the immediate result of our project is something we were very interested in seeing and and are proud of: all of optimizations reduced this problem we tried to solve from taking 653.45 seconds in serial python to just 3.87 seconds in our implementation. \r\nWhile our speed-ups come from many places as described in our results, two that are particularly interesting to look at are the optimizations made in moving our calculations of Simhashes and of Distances in python to cython. To recap, calculating Simhashes goes from taking 158.81 seconds to just 0.1429 seconds (a 1111x speedup) and calculating distances goes from taking 392.14 seconds to 3.34 seconds (a 117x speedup). Both of these likely reap strong benefits from bit-shifting transitioning from the python numpy code to the cython code which is apparently able to execute these instructions much more quickly. In addition, calculating the Simhashes may also be sped up by... //TODO: George or Sami any idea why Simhashes gets so much faster other than the bit-shifting?\r\n\r\n\r\nCluster Similarity\r\n\tOne aspect of this project we were very interested to see results for was how much our method of dimensionality reduction (Simhashing) leading to information loss actually translates into clustering differences. As a reminder, simhashing takes our 300,000 byte vector and translates it into a much smaller fingerprint of 8 bytes for our Simhash64 implementation and half that size (4 bytes) for our Simhash32 implementation. \r\n\tIn order to compare the clusters returned between the Simhash64 and the Simhash32 implementations, we use the Rand Score between the two labelings. According to its documentation, the rand score is a similarity measure between the two clusterings by considering all pairs of samples and then counting the number that are assigned the same label in the two clusterings. The Rand Score has a value of 0.0 for random clustering and 1.0 for exactly the same clustering. Interestingly, between the Simhash64 and Simhash32 functions, we get a Rand Score of 0.45 when we use 10,000 questions and a score of 0.62 when we use 20,000 questions. This is interesting for two reasons: first, both rand scores are somewhat close to about 0.5 meaning that losing half the information between the 64-bit and 32-bit implementations meant having about half as similar clustering. The second interesting insight here is that increasing the number of questions actually increased the similarity of our clustering despite the information loss. This perhaps implies that the larger our sample size, the more accurate our clusterings are overall, which our 64-bit and 32-bit implementations are then presumably approximating. \r\n\t\r\nChunk Size\r\n\tAnother interesting result has to do with the fact that when we reduce chunksize in our parallel implementations to be 1 in all of our loops instead of the standard num_questions / num_threads, we achieve a speedup. We can observe this below for the create_tfs, calculate_idf, and calculate_tfidfs functions.\r\n\r\n\r\n\r\n\r\n\r\nThis is probably because the threads access the same blocks of memory, thus reducing I/O latency. \r\n\r\n***SAMIIIII : I thought I was going to talk about the locking (u-shape curve) and the threading (4 often times does better than 8) in insights but I read your section and you already have this stuff in there. Let me know if you want me to add it here instead. Also, saw you made a note of chunk size differences after I left so let me know if I got what you were saying. \r\nLocking\r\nThreading: 4 vs 8\t\r\n****GEORGEEEE: I know you had some insights you mentioned you wanted to add into the paper, please add any of them here!\r\n\r\n\r\n\r\n\r\n\r\nProposed Extensions and Improvements\r\n\tAlthough we are very happy with the speedups and results we demonstrated over the course of this project, we have also put thought into potential extensions and improvements we could try in the future.\r\n\r\n\r\nOptimizing our Locality Sensitive Hashing\r\n\r\n\tThe approach outlined in this report used Simhash and Hamming Distances, to determine the similarity of two documents in order to do our cluster and de-duplication. While this approaches is vetted in the literature for these purposes, there was a large-scale study conducted by Google in 2006 that explored the use of an alternative Locality Sensitive Hashing scheme, Minhash, to do web document de-duplication and afterwards the company reported using Simhash for duplicate detection on web crawling and Minhash for Google News personalization 2. \r\nGiven the success of Minhash for news personalization (with the idea of making sure to serve representative documents from different clusters), one extension of our project could be to compare the performance and run-time tradeoffs between using Minhash and Simhash on our datasets. Rather than using Hamming distance, Minhash uses Jaccard similarity defined as the following: \r\nIf A and B are two sets we hope to compare:\r\n\r\n\r\n\tIt would be very interesting to compare Minhash and Simhash implementations against each other since calculating distances by far took the longest amount of time, and now we are changing the number of operations we’d conduct in our distance calculation and the nature of the vectors we are comparing, so we could certainly expect to see accuracy and time tradeoffs. \r\n\r\n\r\n2. Gurmeet Singh, Manku; Jain, Arvind; Das Sarma, Anish (2007), \"Detecting near-duplicates for web crawling\", Proceedings of the 16th international conference on World Wide Web. ACM,.\r\nParallelize DBSCAN\r\n\tTo do our clustering we used DBSCAN, a density-based clustering algorithm which we chose because it is a well-cited clustering algorithm and because it can handle both distance and similarity matrices between elements. The pseudocode of the algorithm is below: \r\n\r\n\r\n\r\n\tAs with other clustering algorithms, parallelizing DBSCAN is challenging and would likely require a fair amount of work to think through, so for this project we focused on parallelizing all the other components of the code so that our end results could then be fit into any clustering algorithm and be agnostic of its implementation. However granted that DBSCAN in particular seemed to do fairly well for de-duplication, a future work based on this project could definitely benefit from parallelizing parts of the clustering algorithm and speeding up the time with which we are able to generate our clusters once we have our distance matrix. In order to do this, we could likely benefit from some existing research on this topic 3.\r\n\r\n3 http://conferences.computer.org/sc/2012/papers/1000a053.pdf\r\n\r\n\r\n[Some Section by Sami]\r\n\r\nThere are 4 main steps that the algorithm needs to perform before dbscan can be run:\r\nStep 1- Building a term frequency (TF) vector for every question, where each entry in a TF vector represented the number of times the corresponding word showed up in question.\r\nWe first implemented this code in numpy, which took on average 2.52846384 seconds to run. \r\nWe then implemented it in cython with prange using nogil. \r\nWe observed the following performance, depending on the number of threads:\r\n\r\nAs expected, our best performance was achieved with 8 threads, 0.12 seconds, which is a 21x speed increase over the numpy version. We also notice that the speedup does not increase linearly with the number of threads, and that is probably because of scheduling overhead.\r\n\r\nStep 2: Building an Inverse Document Frequency (IDF) vector where each entry represents the inverse of the number of documents in the corpus the corresponding word shows up in. \r\nWe first implemented this code in numpy, which took 2.751998901 seconds to run.\r\nWe then implemented it in cython with prange using nogil. Here, we have to update a global array of document counts for every word, and ensure that threads do not update the same entries at the same time. We use different granularities of locking to achieve this.\r\nWe observe the following performance depending on the number of threads, keeping the number of locks fixed at 5000 (one lock for every 4 words):\r\n\r\n\r\nAgain, we notice a significant speedup in cython, as well as a speedup due to parallelism. The parallelism speedup is not linear with the number of threads though, again probably due to scheduling overhead, but also due to the fact that more threads are now more likely to wait on each other for locks.\r\n\r\nKeeping the number of threads constant at 8 and varying the granularity of our locks, we observe the following performance:\r\n\r\nAs the number of locks increases from 1 to 10,000, we notice a steady increase in performance (decrease in time). This is due to the fact that the more locks we have, the less likely it is that threads will be idly waiting on each other. However, after 15,000 locks, we start to notice a dip in performance (going from 0.14 to 0.17s). This is because of locking overhead.\r\nOur best result came with 8 threads and 10,000 locks: 0.1396 seconds, instead of the\r\noriginal 2.752 in numpy, which is a speedup of almost 20x.\r\n\r\nStep 3: Building a TF-IDF vector for each question by multiplying the TF vector of each question by the IDF vector. \r\nWe first implemented this code in numpy, which ran in 3.081259 seconds.\r\nWe then implemented it in cython with prange and nogil. \r\nWe also noticed that this operation was a great candidate for us to use AVX SIMD instructions: Instead of taking a TF vector of size M and an IDF vector of size M, and multiplying each ith entry of the TF vector by the ith entry of the IDF vector, we could multiply each group of 8 32-bit floats of the TF vector by the corresponding group of 8 32-bit floats of the IDF vector in the same instruction using SIMD instructions.\r\nHere is our performance as a function of the number of threads with and without SIMD instructions:\r\n\r\n\r\nGEORGEEEE UPDATE THIS, MAYBE IT’S BECAUSE I HAVE THE OLD INTEL CPU, AND YOUR RESULTS ARE DIFFERENT, SO I WONT INTERPRET THEM HERE. IF YOUR RESULTS ARE ACTUALLY DIFFERENT, GIVE THEM TO DEV AND HE’LL PRODUCE ANOTHER SIMILAR CHART, AND THEN YOU CAN INTERPRET THOSE RESULTS HERE\r\n\r\n\r\nStep 4 - Computing similarity / distance between every pair of TF-IDF vectors.\r\nUsually, at the stage of the algorithm, we would compute cosine similarities between every pair (A, B) of TF-IDF vectors according to this formula:\r\n \r\nIn numpy, we achieve this in 253.9 seconds.\r\nThis completely breaks our goal of real-time clustering, even when done in cython. So instead, we use a dimensionality reduction technique called Locality Sensitive Hashing. In particular, we replace every TF-IDF vector with a 64-bit Simhash fingerprint. We want similar question texts to have similar hashes, but traditional hashing techniques can only guarantee that identical question texts will hash to the same value, which is not very useful for us. Simhash is able to achieve similar hashes for similar articles by building the individual bits of a hash from the totals of the hashes of every word in an article.\r\n\r\nHere is the algorithm we used to construct a simhash fingerprint:\r\n\r\n(Taken from http://irl.cse.tamu.edu/people/sadhan/papers/cikm2011.pdf)\r\nWhere u is a question, and each feature in the feature set of u F(u) is a word of the question text. \r\nWith these smaller fingerprints (8 bytes), instead of 300,000 bytes, we can load many article representations in each cache-line, thus reducing our memory accesses, which is a big bottleneck in the computation of pairwise distances. Furthermore, we can now compute distances between each pair of fingerprints much much faster, instead of computing cosine similarities which involve computing the dot product of two 300,000 byte vectors and dividing by the norm of each vector.\r\nIn numpy, it takes us 158.8103 seconds to perform this reduction.\r\nIn cython, we observe the following runtimes when performing this reduction step over our entire corpus:\r\n\r\nWe notice that the reduction if very cheap, even with 20,000 vectors to reduce. \r\nWe notice that the speedup is almost linear here, perhaps because the scheduling overhead’s effect is less felt due to the larger amount of computation involved highlighted by the higher serial time (0.77s) than with our previous computations.\r\nThe 0.1429 seconds we can achieve with 8 threads represents a whopping 1111x speedup over the numpy implementation!\r\n\r\n\r\nNext, we can use these reduced representations of vectors to compute the bitwise distance between them. The distance between two fingerprints is called a Hamming measure and is simply the number of bits that differ between them. We can find that number by XOR’ing the two fingerprints and counting the number of set bits. \r\nXOR is a simple instruction, and we can count the number of set bits of the result by either bit-shifting it and modding by 2 in a loop until we hit 0, or with the following neat function (taken from https://yesteapea.wordpress.com/2013/03/03/counting-the-number-of-set-bits-in-an-integer/):\r\n//for 64 bit numbers\r\nint NumberOfSetBits64(long long i)\r\n{\r\n    i = i - ((i >> 1) & 0x5555555555555555);\r\n    i = (i & 0x3333333333333333) +\r\n        ((i >> 2) & 0x3333333333333333);\r\n    i = ((i + (i >> 4)) & 0x0F0F0F0F0F0F0F0F);\r\n    return (i*(0x0101010101010101))>>56;\r\n}\r\nWe implemented this distance computation first in numpy and ran it in 392.14 seconds.\r\nWe then implemented it in cython with prange and nogil, and obtained the following performance, as a function of the number of threads:\r\n\r\n## Works Cited\r\n\r\n1. http://www.slideshare.net/chartbeat/mockup-infographicv4-27900399\r\n\r\n\r\n### Welcome to GitHub Pages.\r\nThis automatic page generator is the easiest way to create beautiful pages for all of your projects. Author your page content here [using GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/), select a template crafted by a designer, and publish. After your page is generated, you can check out the new `gh-pages` branch locally. If you’re using GitHub Desktop, simply sync your repository and you’ll see the new branch.\r\n\r\n### Designer Templates\r\nWe’ve crafted some handsome templates for you to use. Go ahead and click 'Continue to layouts' to browse through them. You can easily go back to edit your page before publishing. After publishing your page, you can revisit the page generator and switch to another theme. Your Page content will be preserved.\r\n\r\n### Creating pages manually\r\nIf you prefer to not use the automatic generator, push a branch named `gh-pages` to your repository to create a page manually. In addition to supporting regular HTML content, GitHub Pages support Jekyll, a simple, blog aware static site generator. Jekyll makes it easy to create site-wide headers and footers without having to copy them across every page. It also offers intelligent blog support and other advanced templating features.\r\n\r\n### Authors and Contributors\r\nYou can @mention a GitHub username to generate a link to their profile. The resulting `<a>` element will link to the contributor’s GitHub Profile. For example: In 2007, Chris Wanstrath (@defunkt), PJ Hyett (@pjhyett), and Tom Preston-Werner (@mojombo) founded GitHub.\r\n\r\n### Support or Contact\r\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/pages) or [contact support](https://github.com/contact) and we’ll help you sort it out.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}