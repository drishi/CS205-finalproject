{"name":"Cs205-finalproject","tagline":"CS205 final project, parallel computation","body":"\r\n# Project Motivation: Problem We Solve\r\nIn the world today, there has been an explosion of content generation that has began to force us to think more critically about how we can best organize our data. Every single day, more than 92,000 articles are posted to the web <sup> 1 </sup>, leaving social media websites, news aggregators and the like scrambling to figure out what to best serve you and keep you engaged. In addition to helping ensure that users get content that is tailored to their individual interests, clustering such a corpus by content similarity is critical to ensure that the massive amount of redundancy in these articles aren’t repeated in a user’s feed and that they get a diverse set of topics to explore. In another sense, if we find a topic that a user would like to read more on, clustering also enables us to delve deeper into that category and discover more written on it. \r\nUnfortunately, the scale which we have described here does not lend itself well to traditional clustering algorithms as they are extremely computationally intensive and thus cannot run in near real-time (a property we would desire if we were to deploy any of these into a real-world application today). Such algorithms typically run by:\r\nConverting each document in the corpus into a vector. We use TF-IDF for this, which will be described later. \r\nCalculating either similarities (i.e. cosine similarity) or distances between vectors to have a notion of “relatedness”. \r\nUse a clustering algorithm that then uses the measure computed before to organize our corpus. We use DBSCAN for this aspect, an award winning algorithm that does density-based clustering.\r\nWith a large number of documents, all of these steps, and especially calculating similarity/distance, takes a large amount of time. Thus, we decided this would be an ideal problem for us to optimize using parallelism and other techniques (such as dimensionality reduction) to try to bring to near real-time performance so that our implementation could allow a news aggregating service to process and cluster a large set of documents in real-time when a user logs in.\r\n\r\n# Data Described:\r\nIn lieu of articles, our corpus of documents consisted of posts that we pulled from Stack Overflow using their API. We did this because we were initially just interested in doing tag recommendation for questions users had posted, but after working on our project for a few weeks realized that our approach could be used to solve an even harder problem, namely clustering large sets of documents efficiently. Also, the posts were well-formated within the API, we had access to a large number of them, and there were many logical comparisons and similarities to make across posts (i.e. posts about a bug versus posts about style questions, etc). In order to get access to Stack Overflow’s API, we registered ourselves with their system, received an API key and then proceeded to pull 540,000 posts and their related attributes from the website. This was an extremely large set to work with and place into memory, and so we often worked with a subset of the posts of between 20,000 to 40,000 questions. \r\n\r\n\r\nAlthough the posts had several attributes (such as author, score, etc), we focused on just the body of the post as we wanted our approach to be extensible for all other forms of text content. Questions were in html format, so we used BeautifulSoup to extract relevant text and then use the nltk library to stem words.  In order to simplify data processing for later steps, specifically when passing python strings to cython code, we also forced all words to be encoded ascii format.\r\n\r\n# Code and Performance\r\nThere are 4 main steps that the algorithm needs to perform before dbscan can be run:\r\nStep 1- Building a term frequency (TF) vector for every question, where each entry in a TF vector represented the number of times the corresponding word showed up in question.\r\nWe first implemented this code in numpy, which took on average 2.52846384 seconds to run. \r\nWe then implemented it in cython with prange using nogil. \r\nWe observed the following performance, depending on the number of threads:\r\n\r\nAs expected, our best performance was achieved with 8 threads, 0.12 seconds, which is a 21x speed increase over the numpy version. We also notice that the speedup does not increase linearly with the number of threads, and that is probably because of scheduling overhead, which is especially noticeable for such a relatively small computation time.\r\n\r\nStep 2: Building an Inverse Document Frequency (IDF) vector where each entry represents the inverse of the number of documents in the corpus the corresponding word shows up in. \r\nWe first implemented this code in numpy, which took 2.751998901 seconds to run.\r\nWe then implemented it in cython with prange using nogil. Here, we had to update a global array of document counts for every word, and ensure that threads do not update the same entries at the same time. We used different granularities of locking to achieve this.\r\nWe observe the following performance depending on the number of threads, keeping the number of locks fixed at 5000 (one lock for every 4 words):\r\n\r\n\r\nAgain, we notice a significant speedup in cython, as well as a speedup due to parallelism. The parallelism speedup is not linear with the number of threads though, again probably due to scheduling overhead, but also due to the fact that more threads are now more likely to wait on each other for locks.\r\nKeeping the number of threads constant at 8 and varying the granularity of our locks, we observe the following performance:\r\n\r\nAs the number of locks increases from 1 to 10,000, we notice a steady increase in performance (decrease in time). This is due to the fact that the more locks we have, the less likely it is that threads will be idly waiting on each other. However, after 15,000 locks, we start to notice a dip in performance (going from 0.14 to 0.17s). This is because of locking overhead.\r\nOur best result came with 8 threads and 10,000 locks: 0.1396 seconds, instead of the\r\noriginal 2.752 in serial numpy, which is a speedup of almost 20x.\r\n\r\nStep 3: Building a TF-IDF vector for each question by multiplying the TF vector of each question by the IDF vector. \r\nWe first implemented this code in numpy, which ran in 3.081259 seconds.\r\nWe then implemented it in cython with prange and nogil. \r\nWe also noticed that this operation was a great candidate for us to use AVX SIMD instructions: Instead of taking a TF vector of size M and an IDF vector of size M, and multiplying each ith entry of the TF vector by the ith entry of the IDF vector, we could multiply each group of 8 32-bit floats of the TF vector by the corresponding group of 8 32-bit floats of the IDF vector in the same instruction using SIMD instructions.\r\nHere is our performance as a function of the number of threads with and without SIMD instructions:\r\n\r\n\r\nWe notice that parallelism only has a pronounced effect when moving from 1 thread to two threads, after which scheduling overhead begins to outweigh the benefits of parallelism. \r\nWe also notice that AVX SIMD instructions improve performance slightly, but the extent of that improvement diminishes as the number of threads goes up. From what we see, we notice that AVX does slightly better for 1,2, and 4 threads, but not 8 threads.  However, more importantly, looking at these results, we see that AVX gives marginal improvement.  Most likely, this is because of the cost of loading the AVX registers, as we only perform one floating point multiplication in AVX.  If we performed more mathematical operations, we might have seen different results.\r\n\r\nStep 4 - Computing similarity / distance between every pair of TF-IDF vectors.\r\nUsually, at this stage of the algorithm, we would compute cosine similarities between every pair (A, B) of TF-IDF vectors according to this formula:\r\n \r\nIn numpy, we achieve this in 153.9 seconds.\r\nThis completely breaks our goal of real-time clustering, even when done in cython. So instead, we use a dimensionality reduction technique called Locality Sensitive Hashing. In particular, we replace every TF-IDF vector with a 64-bit Simhash fingerprint. We want similar question texts to have similar hashes, but traditional hashing techniques can only guarantee that identical question texts will hash to the same value, which is not very useful for us. Simhash is able to achieve similar hashes for similar articles by building the individual bits of a hash from the totals of the hashes of every word in an article.\r\n\r\nHere is the algorithm we used to construct a simhash fingerprint:\r\n\r\n\r\nWhere u is a question, and each feature in the feature set of u F(u) is a word of the question text. \r\nWith these smaller fingerprints (8 bytes), instead of 300,000 bytes, we can load many article representations in each cache-line, thus reducing our memory accesses, which is a big bottleneck in the computation of pairwise distances. Furthermore, we can now compute distances between each pair of fingerprints much much faster, instead of computing cosine similarities which involve computing the dot product of two 300,000 byte vectors and dividing by the norm of each vector.\r\nIn numpy, it takes us 145.8103 seconds to perform this reduction.\r\nIn cython, we observe the following runtimes when performing this reduction step over our entire corpus:\r\n\r\nWe notice that the reduction if very cheap, even with 20,000 vectors to reduce. \r\nWe notice that the speedup is almost linear here, perhaps because the scheduling overhead’s effect is less felt due to the larger amount of computation involved highlighted by the higher serial time (0.77s) than with our previous computations.\r\nThe 0.1429 seconds we can achieve with 8 threads represents a whopping 1020x speedup over the numpy implementation!\r\n\r\n\r\nNext, we can use these reduced representations of vectors to compute the bitwise distance between them. The distance between two fingerprints is called a Hamming measure and is simply the number of bits that differ between them. We can find that number by XOR’ing the two fingerprints and counting the number of set bits. \r\nXOR is a simple instruction, and we can count the number of set bits of the result by either bit-shifting it and modding by 2 in a loop until we hit 0, or with the following neat function \r\n\r\n    //for 64 bit numbers\r\n    int NumberOfSetBits64(long long i)\r\n    {\r\n        i = i - ((i >> 1) & 0x5555555555555555);\r\n        i = (i & 0x3333333333333333) +\r\n            ((i >> 2) & 0x3333333333333333);\r\n        i = ((i + (i >> 4)) & 0x0F0F0F0F0F0F0F0F);\r\n        return (i*(0x0101010101010101))>>56;\r\n    }\r\nWe implemented this distance computation first in numpy and ran it in 165.956151962 seconds.\r\nWe then implemented it in cython with prange and nogil, and obtained the following performance, as a function of the number of threads:\r\n\r\n\r\nWe can see that this is still an expensive operation, even in cython. We also notice that the speedup due to parallelism is not linear with the number of threads, and in fact, between 4 and 8 threads we see no improvement at all. \r\nWe do however witness a speedup of roughly 330x compared to the numpy code for computing distances.\r\nWhat’s more, the reduction and distance computation added together take a combined 0.64 seconds, which is still much much faster than the original cosine similarity time of 153.9 seconds (240x speedup), and still reasonable for real-time performance.\r\nNote that in practice, the 153.9 seconds taken in numpy to compute cosine similarities are still much lower than the 145+166=311 seconds taken to perform the reduction and compute the distances on fingerprints, and that is mainly because numpy is really bad with bitshift operations, so in practice we would just stick with the cosine similarities when using numpy. In cython however, bitshifting operations are extremely fast and these distances can be computed much faster and are worth the inexpensive reduction time. \r\n\r\nOn aggregate, we are able to run our whole code in 1.07 seconds in cython with parallelism and AVX, instead of 320 seconds in numpy, which is a speedup of around 300x.\r\nEven if we compare our cython code with numpy using cosine similarities (which takes 153 seconds), we still witness a speedup of around 150x, and we are in the range of real-time performance. \r\n\r\n# Screenshot of Example Cluster:\r\n\r\n\r\n# Insights:\r\nOverall, we were extremely pleased with the outcomes of our projects and accordingly have a number of insights we are excited to share. \r\n\r\n\r\n\r\n## Cluster Similarity Between 32-Bit and 64-Bit Simhash\r\nOne aspect of this project we were very interested in investigating was how much our method of dimensionality reduction (Simhashing) leading to information loss actually translates into clustering differences. As a reminder, simhashing takes our 300,000 byte vector and translates it into a much smaller fingerprint of 8 bytes for our Simhash64 implementation and half that size (4 bytes) for our Simhash32 implementation. \r\nIn order to compare the clusters returned between the Simhash64 and the Simhash32 implementations, we use the Rand Score between the two labelings. According to its documentation, the rand score is a similarity measure between the two clusterings by considering all pairs of samples and then counting the number that are assigned the same label in the two clusterings. The Rand Score has a value of 0.0 for random clustering and 1.0 for exactly the same clustering. Interestingly, between the Simhash64 and Simhash32 functions, we get a Rand Score of 0.45 when we use 10,000 questions and a score of 0.62 when we use 20,000 questions. This is interesting for two reasons: first, both rand scores are somewhat close to about 0.5 meaning that losing half the information between the 64-bit and 32-bit implementations meant having about half as similar clustering. The second interesting insight here is that increasing the number of questions actually increased the similarity of our clustering despite the information loss. This change is likely because of how the DBSCAN algorithm functions, in that it requires a set minimum number of points in order to define a cluster which will naturally allow greater classification rates as sample sizes increase.\r\nWe do notice a speedup of 20% when computing 32-bit Simhash fingerprints and the ensuing distances between them. This speedup probably does not warrant the approximately 50% drop in accuracy though.  \r\n\t\r\n## Chunk Size\r\nAnother interesting result has to do with the fact that when we reduce chunksize in our parallel implementations to be 1 in all of our loops instead of the standard num_questions / num_threads, we achieve a speedup. We can observe this below for the create_tfs, calculate_idf, and calculate_tfidfs functions.\r\n\r\n\r\n\r\nThis is probably because the threads access the same blocks of memory, thus reducing I/O latency. \r\n\r\n## Decreasing Marginal Returns to Parallelism\r\nOne set of interesting results we observed was that there were often decreasing marginal returns during our various attempts to optimize our parallelism. When we scaled up the number of threads in our code used from 1 to 2 to 4 to 8 we saw that at certain increments (such as increasing the number of threads from 4 to 8), we actually degraded our performance as the scheduling overhead started to outweigh the benefits of our multi-threaded program. We also noticed this same trend when we included AVX SIMD instructions going from four to eight threads. \r\nThis manner of result was also mirrored when we compared our locking results when calculating IDF. Our program tended to speedup as the number of locks increased until we hit a “sweet spot” of about 10,000 locks, at which point the overhead to acquire and release locks began to gradually displace the performance benefits and actually started to make our program run slower. \r\n\r\n\r\n## Memory Constraints\r\nA final interesting result we noticed is that much of our time in cython isn't spent computing, but instead initializing vectors or matrices to store our results.  These sections weren't parallelized by us, and seem to be bottlenecked by the memory allocator of the sytem.  One thing to note though is that in a system meant to run continuously, we can amortize the cost of memory allocation by keeping these buffers in memory between runs, which would remove this slowdown.\r\n\r\n# Extensions and Improvements\r\nAlthough we are very happy with the speedups and results we demonstrated over the course of this project, we have also put thought into potential extensions and improvements we could try in the future.\r\n\r\n\r\n## Optimizing our Locality Sensitive Hashing\r\n\r\nThe approach outlined in this report used Simhash and Hamming Distances, to determine the similarity of two documents in order to do our cluster and de-duplication. While this approaches is vetted in the literature for these purposes, there was a large-scale study conducted by Google in 2006 that explored the use of an alternative Locality Sensitive Hashing scheme, Minhash, to do web document de-duplication and afterwards the company reported using Simhash for duplicate detection on web crawling and Minhash for Google News personalization <sup> 2 </sup>. \r\nGiven the success of Minhash for news personalization (with the idea of making sure to serve representative documents from different clusters), one extension of our project could be to compare the performance and run-time tradeoffs between using Minhash and Simhash on our datasets. Rather than using Hamming distance, Minhash uses Jaccard similarity defined as the following: \r\nIf A and B are two sets we hope to compare:\r\n\r\n\r\nIt would be very interesting to compare Minhash and Simhash implementations against each other since calculating distances by far took a long amount of time, and now we are changing the number of operations we’d conduct in our distance calculation and the nature of the vectors we are comparing, so we could certainly expect to see accuracy and time tradeoffs. \r\n\r\n\r\n## Parallelize DBSCAN\r\nTo do our clustering we used DBSCAN, a density-based clustering algorithm which we chose because it is a well-cited clustering algorithm and because it can handle both distance and similarity matrices between elements. The pseudocode of the algorithm is below: \r\n\r\n\r\n\r\nAs with other clustering algorithms, parallelizing DBSCAN is challenging and would likely require a fair amount of work to think through, so for this project we focused on parallelizing all the other components of the code so that our end results could then be fit into any clustering algorithm and be agnostic of its implementation. However granted that DBSCAN in particular seemed to do fairly well for de-duplication, a future work based on this project could definitely benefit from parallelizing parts of the clustering algorithm and speeding up the time with which we are able to generate our clusters once we have our distance matrix. In order to do this, we could likely benefit from some existing research on this topic <sup> 3 </sup>.\r\n\r\n\r\n\r\n# Project Takeaways: Challenges, Frustrations, and Fun:\r\nWe really enjoyed witnessing the speedups of our algorithm, as well as the surprisingly good clusters we were able to obtain. We had a lot of uncertainty going into this project about whether there would be questions among our corpus that were redundant enough, and whether the Simhash fingerprints would completely eliminate our accuracy, but our intuition was validated in the end and it was nice to see that a lot of the concepts that we learned in class (multi-threading, SIMD instructions, efficient locking, cache-consciousness, …) could be applied to this real-world application, and combined in a way that has not been achieved in industry yet, to the best of our knowledge.\r\nOne of the most challenging aspects was making our numpy code as efficient as possible in order to ensure that our comparisons were fair and accurate. Another really challenging part was making our initialization code in cython as efficient as possible, as well as creating many different versions of functions, one for every type of Simhash (because of the difference between 32-bit and 64-bit ints), in addition to speeding up our Hamming measure calculations. Integrating c++ hashtables with cython was also very challenging. \r\nThe most frustrating part was dealing with all the compilation errors in cython due to our use of “nogil”. \r\n\r\n\r\n\r\n\r\n\r\n# Works Cited\r\n1. http://www.slideshare.net/chartbeat/mockup-infographicv4-27900399\r\n2. Gurmeet Singh, Manku; Jain, Arvind; Das Sarma, Anish (2007), \"Detecting near-duplicates for web crawling\", Proceedings of the 16th international conference on World Wide Web. ACM,.\r\n3. http://conferences.computer.org/sc/2012/papers/1000a053.pdf\r\n4. http://irl.cse.tamu.edu/people/sadhan/papers/cikm2011.pdf\r\n5. https://yesteapea.wordpress.com/2013/03/03/counting-the-number-of-set-bits-in-an-integer/:\r\n\r\n\r\n### Instructions to use: \r\n\r\nInstall webbrowser in order to visualize the results of our clustering. \r\nThe best way is to ‘pip install webbrowser’\r\nhttps://docs.python.org/2/library/webbrowser.html\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}