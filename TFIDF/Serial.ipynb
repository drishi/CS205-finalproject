{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import os.path\n",
    "sys.path.append(os.path.join('..', 'TFIDF'))\n",
    "sys.path.append(os.path.join('..', 'util'))\n",
    "from timer import Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "questions = pickle.load(open(\"lstOfQuestions.pkl\", \"rb\"))\n",
    "questions = questions[:10000]\n",
    "\n",
    "words_dict = {}\n",
    "question_texts = []\n",
    "total = len(questions)\n",
    "count = 0\n",
    "for question in questions :\n",
    "    if count % (total / 10) == 0:\n",
    "        print count\n",
    "    VALID_TAGS = ['p']\n",
    "\n",
    "    soup = BeautifulSoup(question['body'])\n",
    "\n",
    "    VALID_TAGS = ['p']\n",
    "    INVALID_TAGS = ['code', 'a']\n",
    "\n",
    "    text = ' '.join([s.get_text() for s in soup('p')])\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    words = [stemmer.stem(token.encode('ascii', 'ignore').lower()) for token in tokens]\n",
    "\n",
    "    question_texts.append(words)\n",
    "    \n",
    "    for word in words :\n",
    "        if word in words_dict :\n",
    "            words_dict[word] += 1\n",
    "        else :\n",
    "            words_dict[word] = 1\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "id_hash = {}\n",
    "count = 0\n",
    "for question in questions:\n",
    "    if question['question_id'] not in id_hash:\n",
    "        id_hash[question['question_id']] = 1\n",
    "        count += 1\n",
    "\n",
    "print count\n",
    "\n",
    "# Go through dictionary, removing entries that have a value less\n",
    "# than delta\n",
    "delta = 2\n",
    "words_dict2 = {key: value for key, value in words_dict.items()\n",
    "                 if value >= delta }\n",
    "\n",
    "word_indices = {}\n",
    "index = 0\n",
    "for word in words_dict2 :\n",
    "    word_indices[word] = index\n",
    "    index += 1\n",
    "\n",
    "word_indices = list(word_indices.iteritems())\n",
    "\n",
    "output = open('wordIndices_md.pkl', 'wb')\n",
    "pickle.dump(word_indices, output, -1)\n",
    "output.close()\n",
    "\n",
    "output = open('questionTexts_md.pkl', 'wb')\n",
    "pickle.dump(question_texts, output, -1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for initialization: 0.000574111938477\n",
      "Time for load_questions: 1.19209289551e-06\n",
      "Time for load_indices: 0.0149481296539\n",
      "Time for init_tfs: 1.35794305801\n",
      "Time for create_tfs: 1.69611787796\n",
      "Time for create_idf: 1.34325480461\n",
      "Time for calculate_tfidfs: 0.650036096573\n",
      "Time for calculate_simhashes: 85.1130979061\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "     tfidf = reload(tfidf)\n",
    "except NameError:\n",
    "    import TFIDF_numpy as tfidf\n",
    "\n",
    "try :\n",
    "    word_indices\n",
    "except NameError:\n",
    "    word_indices = pickle.load(open('wordIndices_md.pkl', 'rb'))\n",
    "\n",
    "try :\n",
    "    question_texts\n",
    "except NameError:\n",
    "    question_texts = pickle.load(open('questionTexts_md.pkl', 'rb'))\n",
    "    \n",
    "tfidf.init_globals()\n",
    "tfidf.load_questions(question_texts)\n",
    "tfidf.load_indices(word_indices)\n",
    "tfidf.init_tfs()\n",
    "serial_tfs = tfidf.create_tfs()\n",
    "serial_idf = tfidf.calculate_idf()\n",
    "serial_tfidfs = tfidf.calculate_tfidfs()\n",
    "# tfidf.calculate_tfidf_norms()\n",
    "# tfidf.calculate_cossim(question_texts[0])\n",
    "serial_simhashes = tfidf.calculate_simhashes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling with /usr/local/bin/clang-omp\n",
      "Time for Initialization: 3.09944152832e-05\n",
      "Time for load_questions: 0.0709958076477\n",
      "Time for load_indices: 0.0325329303741\n",
      "Time for init_tfs: 0.963369846344\n",
      "Time for create_tfs: 0.265480041504\n",
      "Time for calculate_idf: 0.206746816635\n",
      "Time for int_tfidfs: 0.926871061325\n",
      "Using AVX\n",
      "Time for calculate_tfidfs: 0.148722171783\n",
      "Time for calculate_simhashes: 0.303247928619\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    tfidf_c = reload(tfidf_c)\n",
    "except :\n",
    "    import TFIDF_cython as tfidf_c\n",
    "\n",
    "try :\n",
    "    word_indices\n",
    "except NameError:\n",
    "    word_indices = pickle.load(open('wordIndices_sm.pkl', 'rb'))\n",
    "\n",
    "try :\n",
    "    question_texts\n",
    "except NameError:\n",
    "    question_texts = pickle.load(open('questionTexts_sm.pkl', 'rb'))\n",
    "\n",
    "# Preprocess for cython code\n",
    "tfidf_c.init_globals(1, True, \"coarse\", 64)\n",
    "tfidf_c.load_questions(question_texts)\n",
    "tfidf_c.load_indices(word_indices)\n",
    "tfidf_c.init_tfs()\n",
    "cython_tfs = np.asarray(tfidf_c.create_tfs())[:,0:len(word_indices)]\n",
    "cython_idf = np.asarray(tfidf_c.calculate_idf(len(word_indices)))[0:len(word_indices)]\n",
    "tfidf_c.init_tfidfs()\n",
    "cython_tfidfs = np.asarray(tfidf_c.calculate_tfidfs())[:,0:len(word_indices)]\n",
    "cython_simhashes = np.asarray(tfidf_c.calculate_simhashes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "3.05905382673e-05\n",
      "0.000547179457161\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "print np.linalg.norm(cython_tfs - serial_tfs)\n",
    "print np.linalg.norm(cython_idf - serial_idf)\n",
    "print np.linalg.norm(cython_tfidfs - serial_tfidfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following are from https://yesteapea.wordpress.com/2013/03/03/counting-the-number-of-set-bits-in-an-integer/\n",
    "def numBits64(i):\n",
    "    i = i - ((i >> np.uint64(1)) & np.uint64(0x5555555555555555))\n",
    "    i = (i & np.uint64(0x3333333333333333)) + ((i >> np.uint64(2)) & np.uint64(0x3333333333333333))\n",
    "    i = ((i + (i >> np.uint64(4))) & np.uint64(0x0F0F0F0F0F0F0F0F))\n",
    "    return (i*(np.uint64(0x0101010101010101)))>>np.uint64(56)\n",
    "\n",
    "def numBits32(i):\n",
    "    i = i - ((i >> 1) & 0x55555555)\n",
    "    i = (i & 0x33333333) + ((i >> 2) & 0x33333333)\n",
    "    i = ((i + (i >> 4)) & 0x0F0F0F0F)\n",
    "    return (i*(0x01010101))>>24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0  percent done\n",
      "10.0  percent done\n",
      "20.0  percent done\n",
      "30.0  percent done\n",
      "40.0  percent done\n",
      "50.0  percent done\n",
      "60.0  percent done\n",
      "70.0  percent done\n",
      "80.0  percent done\n",
      "90.0  percent done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:6: RuntimeWarning: overflow encountered in ulong_scalars\n"
     ]
    }
   ],
   "source": [
    "distances = [[0 for i in xrange(len(cython_simhashes))] for j in xrange(len(cython_simhashes))]\n",
    "for i in xrange(len(cython_simhashes)):\n",
    "    for j in xrange(len(cython_simhashes)):\n",
    "        distances[i][j] = numBits64(cython_simhashes[i] ^ cython_simhashes[j])\n",
    "    if i % 1000 == 0:\n",
    "        print (float(i)/len(cython_simhashes))*100, \" percent done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(metric=\"precomputed\", eps=2, min_samples=3).fit(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9994"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: 1 if x >= 0 else 0, db.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "all_one_labels = []\n",
    "counter = 0\n",
    "for i in xrange(len(db.labels_)):\n",
    "    if db.labels_[i] == -1:\n",
    "        counter += 1\n",
    "    elif db.labels_[i] == 4:\n",
    "        all_one_labels.append(i)\n",
    "print counter\n",
    "print all_one_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in all_one_labels: \n",
    "    print \" \".join(question_texts[i])\n",
    "    print \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
