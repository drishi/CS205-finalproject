{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import os.path\n",
    "sys.path.append(os.path.join('..', 'TFIDF'))\n",
    "sys.path.append(os.path.join('..', 'util'))\n",
    "from timer import Timer\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions = pickle.load(open(\"lstOfQuestions.pkl\", \"rb\"))\n",
    "questions = questions[:2000]\n",
    "\n",
    "words_dict = {}\n",
    "question_texts = []\n",
    "total = len(questions)\n",
    "count = 0\n",
    "for question in questions :\n",
    "    if count % (total / 10) == 0:\n",
    "        print count\n",
    "    VALID_TAGS = ['p']\n",
    "\n",
    "    soup = BeautifulSoup(question['body'])\n",
    "\n",
    "    VALID_TAGS = ['p']\n",
    "    INVALID_TAGS = ['code', 'a']\n",
    "\n",
    "    text = ' '.join([s.get_text() for s in soup('p')])\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    words = [stemmer.stem(token.encode('ascii', 'ignore').lower()) for token in tokens]\n",
    "\n",
    "    question_texts.append(words)\n",
    "    \n",
    "    for word in words :\n",
    "        if word in words_dict :\n",
    "            words_dict[word] += 1\n",
    "        else :\n",
    "            words_dict[word] = 1\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "id_hash = {}\n",
    "count = 0\n",
    "for question in questions:\n",
    "    if question['question_id'] not in id_hash:\n",
    "        id_hash[question['question_id']] = 1\n",
    "        count += 1\n",
    "\n",
    "print count\n",
    "\n",
    "# Go through dictionary, removing entries that have a value less\n",
    "# than delta\n",
    "delta = 2\n",
    "words_dict2 = {key: value for key, value in words_dict.items()\n",
    "                 if value >= delta }\n",
    "\n",
    "word_indices = {}\n",
    "index = 0\n",
    "for word in words_dict2 :\n",
    "    word_indices[word] = index\n",
    "    index += 1\n",
    "\n",
    "word_indices = list(word_indices.iteritems())\n",
    "\n",
    "output = open('wordIndices_sm.pkl', 'wb')\n",
    "pickle.dump(word_indices, output, -1)\n",
    "output.close()\n",
    "\n",
    "output = open('questionTexts_sm.pkl', 'wb')\n",
    "pickle.dump(question_texts, output, -1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "     tfidf = reload(tfidf)\n",
    "except NameError:\n",
    "    import TFIDF_numpy as tfidf\n",
    "\n",
    "try :\n",
    "    word_indices\n",
    "except NameError:\n",
    "    word_indices = pickle.load(open('wordIndices_sm.pkl', 'rb'))\n",
    "\n",
    "try :\n",
    "    question_texts\n",
    "except NameError:\n",
    "    question_texts = pickle.load(open('questionTexts_sm.pkl', 'rb'))\n",
    "    \n",
    "tfidf.init_globals()\n",
    "tfidf.load_questions(question_texts)\n",
    "tfidf.load_indices(word_indices)\n",
    "tfidf.init_tfs()\n",
    "serial_tfs = tfidf.create_tfs()\n",
    "serial_idf = tfidf.calculate_idf()\n",
    "serial_tfidfs = tfidf.calculate_tfidfs()\n",
    "# tfidf.calculate_tfidf_norms()\n",
    "# tfidf.calculate_cossim(question_texts[0])\n",
    "serial_simhashes = tfidf.calculate_simhashes()\n",
    "serial_distances = tfidf.calculate_distances()\n",
    "serial_cossim = tfidf.calculate_cossims()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try :\n",
    "    tfidf_c = reload(tfidf_c)\n",
    "except :\n",
    "    import TFIDF_cython as tfidf_c\n",
    "\n",
    "try :\n",
    "    word_indices\n",
    "except NameError:\n",
    "    word_indices = pickle.load(open('wordIndices_sm.pkl', 'rb'))\n",
    "\n",
    "try :\n",
    "    question_texts\n",
    "except NameError:\n",
    "    question_texts = pickle.load(open('questionTexts_sm.pkl', 'rb'))\n",
    "\n",
    "# Preprocess for cython code\n",
    "tfidf_c.init_globals(1, False, \"coarse\", 64)\n",
    "tfidf_c.load_questions(question_texts)\n",
    "tfidf_c.load_indices(word_indices)\n",
    "tfidf_c.init_tfs()\n",
    "cython_tfs = np.asarray(tfidf_c.create_tfs())[:,0:len(word_indices)]\n",
    "cython_idf = np.asarray(tfidf_c.calculate_idf(len(word_indices)))[0:len(word_indices)]\n",
    "tfidf_c.init_tfidfs()\n",
    "cython_tfidfs = np.asarray(tfidf_c.calculate_tfidfs())[:,0:len(word_indices)]\n",
    "cython_simhashes = np.asarray(tfidf_c.calculate_simhashes())\n",
    "cython_distances = np.asarray(tfidf_c.calculate_distances())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Verification\n",
    "print np.linalg.norm(cython_tfs - serial_tfs)\n",
    "print np.linalg.norm(cython_idf - serial_idf)\n",
    "print np.linalg.norm(cython_tfidfs - serial_tfidfs)\n",
    "print np.linalg.norm(cython_simhashes - serial_simhashes)\n",
    "print np.linalg.norm(cython_distances - serial_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following are from https://yesteapea.wordpress.com/2013/03/03/counting-the-number-of-set-bits-in-an-integer/\n",
    "def numBits64(i):\n",
    "    i = i - ((i >> np.uint64(1)) & np.uint64(0x5555555555555555))\n",
    "    i = (i & np.uint64(0x3333333333333333)) + ((i >> np.uint64(2)) & np.uint64(0x3333333333333333))\n",
    "    i = ((i + (i >> np.uint64(4))) & np.uint64(0x0F0F0F0F0F0F0F0F))\n",
    "    return (i*(np.uint64(0x0101010101010101)))>>np.uint64(56)\n",
    "\n",
    "def numBits32(i):\n",
    "    i = i - ((i >> 1) & 0x55555555)\n",
    "    i = (i & 0x33333333) + ((i >> 2) & 0x33333333)\n",
    "    i = ((i + (i >> 4)) & 0x0F0F0F0F)\n",
    "    return (i*(0x01010101))>>24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([serial_simhashes] * len(serial_simhashes))\n",
    "B = np.copy(A)\n",
    "distances = numBits64(A ^ B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db = DBSCAN(metric=\"precomputed\", eps=2, min_samples=3).fit(cython_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(map(lambda x: 1 if x >= 0 else 0, db.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_one_labels = []\n",
    "counter = 0\n",
    "for i in xrange(len(db.labels_)):\n",
    "    if db.labels_[i] == -1:\n",
    "        counter += 1\n",
    "    elif db.labels_[i] == 4:\n",
    "        all_one_labels.append(i)\n",
    "print counter\n",
    "print all_one_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in all_one_labels: \n",
    "    print \" \".join(question_texts[i])\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now doing it for the 32BIT cython distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try :\n",
    "    tfidf_c = reload(tfidf_c)\n",
    "except :\n",
    "    import TFIDF_cython as tfidf_c\n",
    "\n",
    "try :\n",
    "    word_indices\n",
    "except NameError:\n",
    "    word_indices = pickle.load(open('wordIndices_sm.pkl', 'rb'))\n",
    "\n",
    "try :\n",
    "    question_texts\n",
    "except NameError:\n",
    "    question_texts = pickle.load(open('questionTexts_sm.pkl', 'rb'))\n",
    "\n",
    "# Preprocess for cython code\n",
    "tfidf_c.init_globals(1, False, \"coarse\", 32)\n",
    "tfidf_c.load_questions(question_texts)\n",
    "tfidf_c.load_indices(word_indices)\n",
    "tfidf_c.init_tfs()\n",
    "cython_tfs = np.asarray(tfidf_c.create_tfs())[:,0:len(word_indices)]\n",
    "cython_idf = np.asarray(tfidf_c.calculate_idf(len(word_indices)))[0:len(word_indices)]\n",
    "tfidf_c.init_tfidfs()\n",
    "cython_tfidfs = np.asarray(tfidf_c.calculate_tfidfs())[:,0:len(word_indices)]\n",
    "cython_simhashes = np.asarray(tfidf_c.calculate_simhashes())\n",
    "cython_distances32 = tfidf_c.calculate_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db32 = DBSCAN(metric=\"precomputed\", eps=2, min_samples=3).fit(cython_distances32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(map(lambda x: 1 if x >= 0 else 0, db32.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for j in serial_cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dbCossim = DBSCAN(metric=\"precomputed\", eps=0.03125, min_samples=3).fit(serial_cossim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(map(lambda x: 1 if x >= 0 else 0, dbCossim.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.adjusted_rand_score(db.labels_, db32.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.homogeneity_score(dbCossim.labels_, db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
