{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import sys\n",
    "import os.path\n",
    "sys.path.append(os.path.join('..', 'TFIDF'))\n",
    "sys.path.append(os.path.join('..', 'util'))\n",
    "from timer import Timer\n",
    "from sklearn import metrics\n",
    "import webbrowser\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from HTMLParser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "questions = pickle.load(open(\"lstOfQuestions.pkl\", \"rb\"))\n",
    "questions = questions[:10000]\n",
    "\n",
    "words_dict = {}\n",
    "question_texts = []\n",
    "total = len(questions)\n",
    "count = 0\n",
    "for question in questions :\n",
    "    if count % (total / 10) == 0:\n",
    "        print count\n",
    "    VALID_TAGS = ['p']\n",
    "\n",
    "    soup = BeautifulSoup(question['body'])\n",
    "\n",
    "    VALID_TAGS = ['p']\n",
    "    INVALID_TAGS = ['code', 'a']\n",
    "\n",
    "    text = ' '.join([s.get_text() for s in soup('p')])\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    words = [stemmer.stem(token.encode('ascii', 'ignore').lower()) for token in tokens]\n",
    "\n",
    "    question_texts.append(words)\n",
    "    \n",
    "    for word in words :\n",
    "        if word in words_dict :\n",
    "            words_dict[word] += 1\n",
    "        else :\n",
    "            words_dict[word] = 1\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "id_hash = {}\n",
    "count = 0\n",
    "for question in questions:\n",
    "    if question['question_id'] not in id_hash:\n",
    "        id_hash[question['question_id']] = 1\n",
    "        count += 1\n",
    "\n",
    "print count\n",
    "\n",
    "# Go through dictionary, removing entries that have a value less\n",
    "# than delta\n",
    "delta = 2\n",
    "words_dict2 = {key: value for key, value in words_dict.items()\n",
    "                 if value >= delta }\n",
    "\n",
    "word_indices = {}\n",
    "index = 0\n",
    "for word in words_dict2 :\n",
    "    word_indices[word] = index\n",
    "    index += 1\n",
    "\n",
    "word_indices = list(word_indices.iteritems())\n",
    "\n",
    "output = open('wordIndices_sm.pkl', 'wb')\n",
    "pickle.dump(word_indices, output, -1)\n",
    "output.close()\n",
    "\n",
    "output = open('questionTexts_sm.pkl', 'wb')\n",
    "pickle.dump(question_texts, output, -1)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for initialization: 1.00135803223e-05\n",
      "Time for load_questions: 9.53674316406e-07\n",
      "Time for load_indices: 0.00335597991943\n",
      "Time for init_tfs: 0.0243649482727\n",
      "Time for create_tfs: 0.131696939468\n",
      "Time for create_idf: 0.184086799622\n",
      "Time for calculate_tfidfs: 0.0182771682739\n",
      "Time for calculate_cossims: 0.205546855927\n",
      "Time for calculate_simhashes: 8.99054408073\n",
      "Time for calculate_distances: 0.303831100464\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "     tfidf = reload(tfidf)\n",
    "except NameError:\n",
    "    import TFIDF_numpy as tfidf\n",
    "\n",
    "try :\n",
    "    word_indices\n",
    "except NameError:\n",
    "    word_indices = pickle.load(open('wordIndices_sm.pkl', 'rb'))\n",
    "\n",
    "try :\n",
    "    question_texts\n",
    "except NameError:\n",
    "    question_texts = pickle.load(open('questionTexts_sm.pkl', 'rb'))\n",
    "    \n",
    "tfidf.init_globals()\n",
    "tfidf.load_questions(question_texts)\n",
    "tfidf.load_indices(word_indices)\n",
    "tfidf.init_tfs()\n",
    "serial_tfs = tfidf.create_tfs()\n",
    "serial_idf = tfidf.calculate_idf()\n",
    "serial_tfidfs = tfidf.calculate_tfidfs()\n",
    "# tfidf.calculate_tfidf_norms()\n",
    "serial_cossims = tfidf.calculate_cossims()\n",
    "serial_simhashes = tfidf.calculate_simhashes()\n",
    "serial_distances = tfidf.calculate_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serial_cossims.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling with /usr/local/bin/clang-omp\n",
      "Time for Initialization: 2.98023223877e-05\n",
      "Time for load_questions: 0.111461877823\n",
      "Time for load_indices: 0.0291638374329\n",
      "Time for init_tfs: 0.688503980637\n",
      "Time for create_tfs: 0.144814014435\n",
      "Time for calculate_idf: 0.205336093903\n",
      "Time for int_tfidfs: 0.61073589325\n",
      "Time for calculate_tfidfs: 0.113712072372\n",
      "Time for calculate_simhashes: 0.369334936142\n",
      "Time for calculate distances: 0.837061882019\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    tfidf_c = reload(tfidf_c)\n",
    "except :\n",
    "    import TFIDF_cython as tfidf_c\n",
    "\n",
    "try :\n",
    "    word_indices\n",
    "except NameError:\n",
    "    word_indices = pickle.load(open('wordIndices_sm.pkl', 'rb'))\n",
    "\n",
    "try :\n",
    "    question_texts\n",
    "except NameError:\n",
    "    question_texts = pickle.load(open('questionTexts_sm.pkl', 'rb'))\n",
    "\n",
    "# Preprocess for cython code\n",
    "tfidf_c.init_globals(1, False, \"coarse\", 64)\n",
    "tfidf_c.load_questions(question_texts)\n",
    "tfidf_c.load_indices(word_indices)\n",
    "tfidf_c.init_tfs()\n",
    "cython_tfs = np.asarray(tfidf_c.create_tfs())[:,0:len(word_indices)]\n",
    "cython_idf = np.asarray(tfidf_c.calculate_idf(len(word_indices)))[0:len(word_indices)]\n",
    "tfidf_c.init_tfidfs()\n",
    "cython_tfidfs = np.asarray(tfidf_c.calculate_tfidfs())[:,0:len(word_indices)]\n",
    "cython_simhashes = np.asarray(tfidf_c.calculate_simhashes())\n",
    "cython_distances = np.asarray(tfidf_c.calculate_distances())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "5.66585172214e-06\n",
      "8.73883976884e-05\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "print np.linalg.norm(cython_tfs - serial_tfs)\n",
    "print np.linalg.norm(cython_idf - serial_idf)\n",
    "print np.linalg.norm(cython_tfidfs - serial_tfidfs)\n",
    "print np.linalg.norm(cython_simhashes - serial_simhashes)\n",
    "print np.linalg.norm(cython_distances - serial_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The following are from https://yesteapea.wordpress.com/2013/03/03/counting-the-number-of-set-bits-in-an-integer/\n",
    "def numBits64(i):\n",
    "    i = i - ((i >> np.uint64(1)) & np.uint64(0x5555555555555555))\n",
    "    i = (i & np.uint64(0x3333333333333333)) + ((i >> np.uint64(2)) & np.uint64(0x3333333333333333))\n",
    "    i = ((i + (i >> np.uint64(4))) & np.uint64(0x0F0F0F0F0F0F0F0F))\n",
    "    return (i*(np.uint64(0x0101010101010101)))>>np.uint64(56)\n",
    "\n",
    "def numBits32(i):\n",
    "    i = i - ((i >> 1) & 0x55555555)\n",
    "    i = (i & 0x33333333) + ((i >> 2) & 0x33333333)\n",
    "    i = ((i + (i >> 4)) & 0x0F0F0F0F)\n",
    "    return (i*(0x01010101))>>24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = np.array([serial_simhashes] * len(serial_simhashes))\n",
    "B = np.copy(A)\n",
    "distances = numBits64(A ^ B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db64 = DBSCAN(metric=\"precomputed\", eps=2, min_samples=3).fit(cython_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below will just see how many of our documents were NOT labeled as \"noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(lambda x: 1 if x >= 0 else 0, db64.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many labels do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1, 0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(db64.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now doing it for the 32BIT cython distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try :\n",
    "    tfidf_c = reload(tfidf_c)\n",
    "except :\n",
    "    import TFIDF_cython as tfidf_c\n",
    "\n",
    "try :\n",
    "    word_indices\n",
    "except NameError:\n",
    "    word_indices = pickle.load(open('wordIndices_sm.pkl', 'rb'))\n",
    "\n",
    "try :\n",
    "    question_texts\n",
    "except NameError:\n",
    "    question_texts = pickle.load(open('questionTexts_sm.pkl', 'rb'))\n",
    "\n",
    "# Preprocess for cython code\n",
    "tfidf_c.init_globals(1, False, \"coarse\", 32)\n",
    "tfidf_c.load_questions(question_texts)\n",
    "tfidf_c.load_indices(word_indices)\n",
    "tfidf_c.init_tfs()\n",
    "cython_tfs = np.asarray(tfidf_c.create_tfs())[:,0:len(word_indices)]\n",
    "cython_idf = np.asarray(tfidf_c.calculate_idf(len(word_indices)))[0:len(word_indices)]\n",
    "tfidf_c.init_tfidfs()\n",
    "cython_tfidfs = np.asarray(tfidf_c.calculate_tfidfs())[:,0:len(word_indices)]\n",
    "cython_simhashes = np.asarray(tfidf_c.calculate_simhashes())\n",
    "cython_distances32 = tfidf_c.calculate_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db32 = DBSCAN(metric=\"precomputed\", eps=2, min_samples=3).fit(cython_distances32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(map(lambda x: 1 if x >= 0 else 0, db32.labels_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, lets conduct DBSCAN on our Cosine Similarity matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbCossim = DBSCAN(metric=\"precomputed\", eps=0.9, min_samples=3).fit(serial_cossims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(map(lambda x: 1 if x >= 0 else 0, dbCossim.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(dbCossim.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max(serial_cossims[0][1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.adjusted_rand_score(db.labels_, db32.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.adjusted_rand_score(dbCossim.labels_, db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.homogeneity_score(dbCossim.labels_, db.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code that takes in a db instances and displays a particular cluster for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def displayCluster(db, how_near=2, how_many=3, label=0, max_pages=5):\n",
    "    all_one_labels = []\n",
    "    counter = 0\n",
    "    for i in xrange(len(db.labels_)):\n",
    "        if db.labels_[i] == -1:\n",
    "            counter += 1\n",
    "        elif db.labels_[i] == label:\n",
    "            all_one_labels.append(i)\n",
    "    counter = 0 \n",
    "    for q in all_one_labels:\n",
    "        lnk = questions[q]['link']\n",
    "        if counter >= max_pages:\n",
    "            break\n",
    "        counter += 1\n",
    "        webbrowser.open_new(lnk)\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "displayCluster(db64, label=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[191, 6760, 6906]\n"
     ]
    }
   ],
   "source": [
    "all_one_labels = []\n",
    "for i in xrange(len(db64.labels_)):\n",
    "    if db64.labels_[i] == -1:\n",
    "        counter += 1\n",
    "    elif db64.labels_[i] == 5:\n",
    "        all_one_labels.append(i)\n",
    "print all_one_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i chang the color of the powershel and now i ca n't chang the color of the input text , is alway yellow . 1** i chang the color of the background and the color of the text chang color 2** the color of the background chang right but in the display text the color look yellow alway chang i can do someth to reset the color ?\n",
      "\n",
      "there are 16 color in c # : is there ani way i can add more color ( like brown , orang , etc . ) ?\n",
      "\n",
      "i want the color scheme to span complet across the termin boundari . i am use color scheme scroller plugin to switch between differ theme . i have upload a .gif file so that you can clearli see what i want to get fix . vim colorschem doe n't complet chang the color of editor . there are some termin color 's border left around the vim 's overridden color scheme . how would i fix it . pleas check the imag on thi link . stackoverflow doe n't allow upload an imag > 2mb\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in all_one_labels: \n",
    "    print \" \".join(question_texts[i])\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
